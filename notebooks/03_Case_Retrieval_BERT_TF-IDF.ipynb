{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNRZ6+7nzMUsEjlphlSLy2V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# 03_Case_Retrieval_BERT_TF-IDF.ipynb\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.metrics.pairwise import cosine_similarity\n","import os\n","import json\n","from google.colab import drive"],"metadata":{"id":"8YixvMHva_Kk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0UDbqr-gQog","executionInfo":{"status":"ok","timestamp":1750945020288,"user_tz":-420,"elapsed":21304,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"outputId":"2c811093-d2bc-4d74-c28b-c94ab8833b13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# --- Konfigurasi Path (sesuaikan dengan struktur proyek Anda) ---\n","BASE_DRIVE_PATH = \"/content/drive/MyDrive/Semester 6/PK/UAS\" # Sesuaikan jika berbeda\n","PATH_PROCESSED_DATA = os.path.join(BASE_DRIVE_PATH, \"data/processed\")\n","PATH_EVAL_DATA = os.path.join(BASE_DRIVE_PATH, \"data/eval\")\n","PATH_MODELS_CACHE = os.path.join(BASE_DRIVE_PATH, \"models_cache\") # Opsional, untuk cache model Hugging Face\n","\n","# Membuat direktori jika belum ada\n","os.makedirs(PATH_EVAL_DATA, exist_ok=True)\n","os.makedirs(PATH_MODELS_CACHE, exist_ok=True) # Opsional\n","\n","# File input dari Tahap 2\n","CASES_REPRESENTED_CSV = os.path.join(PATH_PROCESSED_DATA, \"cases_represented.csv\") # Atau .json jika Anda simpan sbg JSON\n","\n","# File output untuk tahap ini\n","QUERIES_JSON_FILE = os.path.join(PATH_EVAL_DATA, \"queries.json\")\n","CASE_EMBEDDINGS_FILE = os.path.join(PATH_PROCESSED_DATA, \"case_embeddings_bert.npy\") # Untuk menyimpan embeddings\n","CASE_IDS_FILE = os.path.join(PATH_PROCESSED_DATA, \"case_ids_bert.json\") # Untuk menyimpan urutan case_id sesuai embeddings\n"],"metadata":{"id":"RRf6_yKWa-04"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 1. Muat Data Kasus dari Tahap 2 ---\n","print(f\"Memuat data kasus dari: {CASES_REPRESENTED_CSV}\")\n","try:\n","    df_cases = pd.read_csv(CASES_REPRESENTED_CSV)\n","except FileNotFoundError:\n","    print(f\"Error: File {CASES_REPRESENTED_CSV} tidak ditemukan. Pastikan Tahap 2 sudah dijalankan.\")\n","    exit()\n","\n","if 'text_full' not in df_cases.columns or 'case_id' not in df_cases.columns:\n","    print(\"Error: Kolom 'text_full' atau 'case_id' tidak ditemukan dalam CSV. Periksa output Tahap 2.\")\n","    exit()\n","\n","# Hilangkan baris dengan teks kosong jika ada, karena tidak bisa di-embed\n","df_cases.dropna(subset=['text_full'], inplace=True)\n","df_cases = df_cases[df_cases['text_full'].str.strip() != \"\"]\n","\n","if df_cases.empty:\n","    print(\"Tidak ada kasus dengan teks yang valid untuk diproses. Hentikan.\")\n","    exit()\n","\n","print(f\"Data kasus berhasil dimuat. Jumlah kasus yang akan diproses: {len(df_cases)}\")\n","print(df_cases.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_HXumkBxbCXv","executionInfo":{"status":"ok","timestamp":1750945023084,"user_tz":-420,"elapsed":930,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"outputId":"3340e48d-51f8-445e-ecc1-aa95f8ce23f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Memuat data kasus dari: /content/drive/MyDrive/Semester 6/PK/UAS/data/processed/cases_represented.csv\n","Data kasus berhasil dimuat. Jumlah kasus yang akan diproses: 18\n","   case_id             no_perkara     tanggal  \\\n","0        1   28/Pdt.G/2020/PN Sdw         NaN   \n","1        2  229/Pdt.G/2023/PN Nga         NaN   \n","2        3   40/Pdt.G/2019/PN Ckr  2019-04-25   \n","3        4    9/Pdt.G/2025/PA.Sdk         NaN   \n","4        7   27/Pdt.G/2023/PN Nga         NaN   \n","\n","                            jenis_perkara  \\\n","0        Perdata Perdata Agama Perceraian   \n","1        Perdata Perdata Agama Perceraian   \n","2        Perdata Perdata Agama Perceraian   \n","3  Perdata Agama Perdata Agama Perceraian   \n","4        Perdata Perdata Agama Perceraian   \n","\n","                                   pihak  \\\n","0  Pihak tidak teridentifikasi dari teks   \n","1  Pihak tidak teridentifikasi dari teks   \n","2  Penggugat/Pemohon: MERY YANTI L. GAOL   \n","3             Penggugat/Pemohon: PEMOHON   \n","4  Pihak tidak teridentifikasi dari teks   \n","\n","                                     ringkasan_fakta  \\\n","0  Menimbang, bahwa Penggugat dengan surat gugata...   \n","1  Menimbang, bahwa Penggugat dengan surat gugata...   \n","2                                                NaN   \n","3  Bahwa Pemohon dengan surat permohonannya terta...   \n","4  Menimbang, bahwa Penggugat dengan surat gugata...   \n","\n","                                    pasal_diterapkan  \\\n","0  ['Pasal 149 ayat (1) Rbg/125 ayat (1) HIR Junc...   \n","1  ['Pasal 19Peraturan Pemerintah Nomor 9 Tahun 1...   \n","2                                                 []   \n","3  ['Pasal 1 huruf (a) dan huruf (f), serta Pasal...   \n","4  ['Pasal 19 Peraturan Pemerintah Nomor 9 Tahun ...   \n","\n","                                       argumen_hukum amar_putusan  \\\n","0  Menimbang, bahwa pada hari sidang yang telah d...    Lain-lain   \n","1  Menimbang, bahwa sekalipun telah dipanggil sec...    Lain-lain   \n","2  Menimbang, bahwa Penggugat dengan surat gugata...        Gugur   \n","3  Menimbang, bahwa untuk menguatkan dalil-dalil ...    Lain-lain   \n","4  Menimbang, bahwa sekalipun Tergugat telah dipa...    Lain-lain   \n","\n","   jumlah_kata                                          text_full  \\\n","0         6667  Direktori Putusan Mahkamah Agung Republik Indo...   \n","1         3234  Direktori Putusan Mahkamah Agung Republik Indo...   \n","2         1378  Direktori Putusan Mahkamah Agung Republik Indo...   \n","3         5875  Direktori Putusan Mahkamah Agung Republik Indo...   \n","4         3483  Direktori Putusan Mahkamah Agung Republik Indo...   \n","\n","                                         link_sumber  tahun_dokumen  \n","0  https://putusan3.mahkamahagung.go.id/direktori...           2020  \n","1  https://putusan3.mahkamahagung.go.id/direktori...           2023  \n","2  https://putusan3.mahkamahagung.go.id/direktori...           2019  \n","3  https://putusan3.mahkamahagung.go.id/direktori...           2025  \n","4  https://putusan3.mahkamahagung.go.id/direktori...           2023  \n"]}]},{"cell_type":"markdown","source":["# BERT"],"metadata":{"id":"nsMasQRuc0TF"}},{"cell_type":"code","source":["# --- 2. Setup Model IndoBERT ---\n","# Tentukan device (GPU jika tersedia, jika tidak CPU)\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Menggunakan device: {DEVICE}\")\n","\n","# Nama model IndoBERT (sesuai rekomendasi PDF atau pilih yang lain jika perlu) [cite: 198]\n","MODEL_NAME = 'indobenchmark/indobert-base-p1' # [cite: 198]\n","print(f\"Memuat tokenizer dan model untuk: {MODEL_NAME}...\")\n","\n","try:\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=PATH_MODELS_CACHE)\n","    model = AutoModel.from_pretrained(MODEL_NAME, cache_dir=PATH_MODELS_CACHE)\n","    model.to(DEVICE) # Pindahkan model ke device (GPU/CPU)\n","    model.eval() # Set model ke mode evaluasi\n","    print(\"Tokenizer dan model berhasil dimuat.\")\n","except Exception as e:\n","    print(f\"Error saat memuat model atau tokenizer: {e}\")\n","    print(\"Pastikan Anda memiliki koneksi internet untuk mengunduh model jika belum ada di cache.\")\n","    print(\"Jika menggunakan Colab, pastikan runtime memiliki akses internet.\")\n","    exit()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwjBeGlLbGyY","executionInfo":{"status":"ok","timestamp":1750945052648,"user_tz":-420,"elapsed":29568,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"outputId":"eed0e717-1589-4de8-9258-1c3f2034e3af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Menggunakan device: cuda\n","Memuat tokenizer dan model untuk: indobenchmark/indobert-base-p1...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizer dan model berhasil dimuat.\n"]}]},{"cell_type":"code","source":["# --- 3. Fungsi untuk Menghasilkan Embeddings ---\n","def get_bert_embedding(text, model, tokenizer, device, max_length=512):\n","    \"\"\"\n","    Menghasilkan embedding untuk teks menggunakan model BERT (IndoBERT).\n","    Menggunakan strategi Mean Pooling pada last hidden states.\n","    \"\"\"\n","    if not isinstance(text, str) or text.strip() == \"\":\n","        # Kembalikan zero vector atau handle error jika teks tidak valid\n","        # Untuk IndoBERT base, hidden size biasanya 768\n","        return torch.zeros(model.config.hidden_size).cpu().numpy()\n","\n","    encoded_input = tokenizer(\n","        text,\n","        padding='max_length',    # Pad to max_length\n","        truncation=True,         # Truncate to max_length\n","        max_length=max_length,\n","        return_attention_mask=True,\n","        return_tensors='pt'      # Return PyTorch tensors\n","    )\n","    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n","\n","    with torch.no_grad(): # Tidak perlu menghitung gradien saat inferensi\n","        outputs = model(**encoded_input)\n","        last_hidden_states = outputs.last_hidden_state\n","\n","    # Mean Pooling: rata-rata token embeddings berdasarkan attention mask\n","    attention_mask = encoded_input['attention_mask']\n","    mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n","    sum_embeddings = torch.sum(last_hidden_states * mask_expanded, 1)\n","    sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9) # Hindari pembagian dengan nol\n","    mean_pooled = sum_embeddings / sum_mask\n","\n","    return mean_pooled.cpu().numpy().flatten() # Kembali ke CPU, ubah ke NumPy array 1D"],"metadata":{"id":"m5pCQ9xLbLz6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 4. Generate dan Simpan Embeddings untuk Case Base ---\n","# Cek apakah embeddings sudah ada untuk menghindari komputasi ulang\n","if os.path.exists(CASE_EMBEDDINGS_FILE) and os.path.exists(CASE_IDS_FILE):\n","    print(f\"Memuat embeddings yang sudah ada dari {CASE_EMBEDDINGS_FILE}...\")\n","    case_embeddings = np.load(CASE_EMBEDDINGS_FILE)\n","    with open(CASE_IDS_FILE, 'r') as f:\n","        case_ids_for_embeddings = json.load(f)\n","    print(f\"Embeddings dan case_ids berhasil dimuat. Jumlah: {len(case_ids_for_embeddings)}\")\n","\n","    # Pastikan jumlah case_ids cocok dengan jumlah baris embeddings\n","    if len(case_ids_for_embeddings) != case_embeddings.shape[0]:\n","        print(\"Peringatan: Jumlah case_ids tidak cocok dengan jumlah embeddings! Akan mengenerate ulang.\")\n","        generate_new_embeddings = True\n","    else:\n","        generate_new_embeddings = False\n","else:\n","    generate_new_embeddings = True\n","\n","if generate_new_embeddings:\n","    print(\"Menghasilkan embeddings untuk case base (mungkin butuh waktu)...\")\n","    # Gunakan kolom 'text_full' dari df_cases\n","    # Pastikan df_cases memiliki case_id yang unik dan teks yang valid\n","\n","    embeddings_list = []\n","    case_ids_for_embeddings = []\n","\n","    for index, row in df_cases.iterrows():\n","        case_id = row['case_id']\n","        text_to_embed = str(row['text_full']) # Pastikan berupa string\n","\n","        if not text_to_embed.strip():\n","            print(f\"Peringatan: Teks kosong untuk case_id {case_id}. Dilewati.\")\n","            continue\n","\n","        print(f\"  Processing case_id: {case_id}...\")\n","        embedding = get_bert_embedding(text_to_embed, model, tokenizer, DEVICE)\n","        embeddings_list.append(embedding)\n","        case_ids_for_embeddings.append(case_id)\n","\n","    if not embeddings_list:\n","        print(\"Error: Tidak ada embeddings yang berhasil digenerate. Periksa teks input.\")\n","        exit()\n","\n","    case_embeddings = np.array(embeddings_list)\n","\n","    print(f\"Shape dari case_embeddings: {case_embeddings.shape}\")\n","\n","    # Simpan embeddings dan urutan case_id\n","    try:\n","        np.save(CASE_EMBEDDINGS_FILE, case_embeddings)\n","        print(f\"Case embeddings berhasil disimpan ke: {CASE_EMBEDDINGS_FILE}\")\n","        with open(CASE_IDS_FILE, 'w') as f:\n","            json.dump(case_ids_for_embeddings, f)\n","        print(f\"Urutan Case IDs untuk embeddings disimpan ke: {CASE_IDS_FILE}\")\n","    except Exception as e:\n","        print(f\"Error saat menyimpan embeddings atau case_ids: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BhAovMSxbSSq","executionInfo":{"status":"ok","timestamp":1750945053520,"user_tz":-420,"elapsed":834,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"outputId":"6487f2a0-096d-4d14-b98f-ead70e70e49a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Memuat embeddings yang sudah ada dari /content/drive/MyDrive/Semester 6/PK/UAS/data/processed/case_embeddings_bert.npy...\n","Embeddings dan case_ids berhasil dimuat. Jumlah: 18\n"]}]},{"cell_type":"code","source":["# --- 5. Implementasi Fungsi Retrieval ---\n","def retrieve_cases_bert(query_text, k=5):\n","    \"\"\"\n","    Menerima query teks, menghasilkan embeddingnya, menghitung kemiripan\n","    dengan case base, dan mengembalikan top-k case_id yang paling mirip.\n","    \"\"\"\n","    if not query_text.strip():\n","        print(\"Query teks kosong.\")\n","        return []\n","\n","    print(f\"\\nMelakukan retrieval untuk query: \\\"{query_text[:100]}...\\\"\")\n","\n","    # 1. Preprocessing query (jika ada, tapi biasanya tokenizer BERT menangani)\n","    # query_processed = preprocess_text_for_bert(query_text) # Jika ada fungsi khusus\n","\n","    # 2. Hasilkan embedding untuk query\n","    print(\"  Menghasilkan embedding untuk query...\")\n","    query_embedding = get_bert_embedding(query_text, model, tokenizer, DEVICE)\n","    query_embedding = query_embedding.reshape(1, -1) # Reshape untuk cosine_similarity\n","\n","    # 3. Hitung cosine similarity\n","    print(\"  Menghitung cosine similarity...\")\n","    # case_embeddings sudah berupa (n_cases, embedding_dim)\n","    # query_embedding sudah berupa (1, embedding_dim)\n","    similarities = cosine_similarity(query_embedding, case_embeddings)\n","\n","    # similarities adalah array 2D [[s1, s2, ...]], ambil baris pertama\n","    similarities = similarities[0]\n","\n","    # 4. Dapatkan top-k indices\n","    # argsort mengembalikan indeks yang akan mengurutkan array (dari terkecil ke terbesar)\n","    # jadi kita ambil dari belakang untuk skor tertinggi\n","    top_k_indices = np.argsort(similarities)[-k:][::-1] # Ambil k terakhir, lalu balik urutannya\n","\n","    # 5. Kembalikan case_id yang sesuai\n","    top_k_case_ids = [case_ids_for_embeddings[i] for i in top_k_indices]\n","    top_k_scores = [similarities[i] for i in top_k_indices]\n","\n","    print(\"  Top-k hasil retrieval (case_id: score):\")\n","    for case_id, score in zip(top_k_case_ids, top_k_scores):\n","        print(f\"    {case_id}: {score:.4f}\")\n","\n","    return top_k_case_ids, top_k_scores"],"metadata":{"id":"uMZN8ZhbbVOv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 6. Persiapan dan Penyimpanan Test Queries (queries.json) ---\n","# [cite: 200]\n","# Buat beberapa contoh query dan tentukan secara manual case_id yang relevan dari data Anda\n","# Ini akan digunakan juga di Tahap Evaluasi.\n","# Anda perlu menyesuaikan query_text dan relevant_case_ids berdasarkan data Anda.\n","\n","test_queries_data = [\n","    {\n","        \"query_id\": \"Q001\",\n","        \"query_text\": \"Suami meninggalkan istri dan anak lebih dari dua tahun tanpa nafkah dan kabar berita.\",\n","        \"relevant_case_ids\": [2, 4, 10, 12, 14, 22, 23, 29, 34, 35] # Isi dengan case_id yang relevan dari data Anda setelah inspeksi\n","    },\n","    {\n","        \"query_id\": \"Q002\",\n","        \"query_text\": \"Terjadi perselisihan dan pertengkaran terus menerus dalam rumah tangga sehingga tidak ada harapan rukun kembali.\",\n","        \"relevant_case_ids\": [1, 2, 3, 4, 7, 10, 12, 14, 21, 22, 23, 25, 29, 30, 32, 33, 34, 35] # Isi dengan case_id yang relevan\n","    },\n","    {\n","        \"query_id\": \"Q003\",\n","        \"query_text\": \"Salah satu pihak melakukan penganiayaan.\", # Ganti dengan domain yang relevan jika ini perceraian\n","        \"relevant_case_ids\": [1, 2, 7, 10, 14, 21]\n","    }\n","]\n","\n","# Jika Anda sudah punya case_ids_for_embeddings, Anda bisa pilih beberapa untuk jadi ground truth\n","# Contoh: jika case_ids_for_embeddings = [101, 102, 103, 104, 105, ...] (ID kasus Anda)\n","# Maka Anda bisa set: test_queries_data[0]['relevant_case_ids'] = [102, 105] (misalnya)\n","\n","print(f\"\\nMenyimpan contoh queries ke {QUERIES_JSON_FILE}...\")\n","try:\n","    with open(QUERIES_JSON_FILE, 'w', encoding='utf-8') as f:\n","        json.dump(test_queries_data, f, indent=4, ensure_ascii=False)\n","    print(\"Contoh queries berhasil disimpan.\")\n","    print(f\"Silakan EDIT file '{QUERIES_JSON_FILE}' untuk mengisi 'relevant_case_ids' yang sesuai dengan data Anda.\")\n","except Exception as e:\n","    print(f\"Error saat menyimpan queries.json: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J5mXvi8ZbznR","executionInfo":{"status":"ok","timestamp":1750945054179,"user_tz":-420,"elapsed":633,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"outputId":"e39e76bb-2ea7-4274-8577-8a489daabea8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Menyimpan contoh queries ke /content/drive/MyDrive/Semester 6/PK/UAS/data/eval/queries.json...\n","Contoh queries berhasil disimpan.\n","Silakan EDIT file '/content/drive/MyDrive/Semester 6/PK/UAS/data/eval/queries.json' untuk mengisi 'relevant_case_ids' yang sesuai dengan data Anda.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gheUznZCaw-2","executionInfo":{"status":"ok","timestamp":1750945054821,"user_tz":-420,"elapsed":628,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"outputId":"530f756c-1425-4a7c-95f3-da4d3539cbae"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Pengujian Awal Fungsi Retrieval ---\n","Menguji dengan Query ID: Q001\n","\n","Melakukan retrieval untuk query: \"Suami meninggalkan istri dan anak lebih dari dua tahun tanpa nafkah dan kabar berita....\"\n","  Menghasilkan embedding untuk query...\n","  Menghitung cosine similarity...\n","  Top-k hasil retrieval (case_id: score):\n","    30: 0.4379\n","    2: 0.4276\n","    29: 0.4036\n","    7: 0.3957\n","    23: 0.3881\n","  ID Kasus yang berhasil di-retrieve: [30, 2, 29, 7, 23]\n","  Skor kemiripan: [np.float32(0.43792742), np.float32(0.42757782), np.float32(0.40357864), np.float32(0.39570495), np.float32(0.3880915)]\n","  Ground truth relevant_case_ids: [2, 4, 10, 12, 14, 22, 23, 29, 34, 35]\n","\n","--- Tahap 3 Selesai (Implementasi Dasar Retrieval dengan BERT) ---\n","Output utama tahap ini adalah fungsi retrieve_cases_bert() yang teruji secara kualitatif\n","dan file queries.json yang perlu Anda lengkapi ground truth-nya.\n","Embeddings kasus disimpan di: /content/drive/MyDrive/Semester 6/PK/UAS/data/processed/case_embeddings_bert.npy\n"]}],"source":["# --- 7. Pengujian Awal Fungsi Retrieval ---\n","print(\"\\n--- Pengujian Awal Fungsi Retrieval ---\")\n","if os.path.exists(QUERIES_JSON_FILE):\n","    try:\n","        with open(QUERIES_JSON_FILE, 'r', encoding='utf-8') as f:\n","            test_queries_for_run = json.load(f)\n","\n","        # Uji dengan query pertama dari file\n","        if test_queries_for_run:\n","            sample_query = test_queries_for_run[0]\n","            print(f\"Menguji dengan Query ID: {sample_query['query_id']}\")\n","            retrieved_ids, retrieved_scores = retrieve_cases_bert(sample_query['query_text'], k=5)\n","            print(f\"  ID Kasus yang berhasil di-retrieve: {retrieved_ids}\")\n","            print(f\"  Skor kemiripan: {retrieved_scores}\")\n","            if sample_query.get('relevant_case_ids'): # Jika Anda sudah mengisi ground truth\n","                 print(f\"  Ground truth relevant_case_ids: {sample_query['relevant_case_ids']}\")\n","        else:\n","            print(\"File queries.json kosong atau tidak valid.\")\n","\n","    except Exception as e:\n","        print(f\"Error saat memuat atau menjalankan query dari queries.json: {e}\")\n","else:\n","    print(f\"File {QUERIES_JSON_FILE} tidak ditemukan. Pengujian awal dilewati.\")\n","\n","print(\"\\n--- Tahap 3 Selesai (Implementasi Dasar Retrieval dengan BERT) ---\")\n","print(\"Output utama tahap ini adalah fungsi retrieve_cases_bert() yang teruji secara kualitatif\")\n","print(\"dan file queries.json yang perlu Anda lengkapi ground truth-nya.\")\n","print(f\"Embeddings kasus disimpan di: {CASE_EMBEDDINGS_FILE}\")"]},{"cell_type":"markdown","source":["# TF-IDF"],"metadata":{"id":"ZIqYPTN2c5Bb"}},{"cell_type":"code","source":["# ==============================================================================\n","# TF-IDF + COSINE SIMILARITY\n","# ==============================================================================\n","# Bagian ini mengimplementasikan metode retrieval statistik sebagai perbandingan.\n","\n","import nltk\n","import re\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import joblib # Untuk menyimpan dan memuat model/vectorizer\n","\n","# --- 1. Persiapan dan Preprocessing Teks untuk TF-IDF ---\n","print(\"\\n--- Memulai Pendekatan 1: TF-IDF + Cosine Similarity ---\")\n","\n","# Unduh daftar stopwords bahasa Indonesia dari NLTK jika belum ada\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError:\n","    print(\"Mengunduh package 'stopwords' untuk NLTK...\")\n","    nltk.download('stopwords', quiet=True)\n","    print(\"'stopwords' berhasil diunduh.\")\n","\n","# Ambil daftar stopwords bahasa Indonesia\n","stop_words = list(stopwords.words('indonesian'))\n","print(f\"Jumlah stopwords bahasa Indonesia: {len(stop_words)}\")\n","\n","def preprocess_for_tfidf(text):\n","    \"\"\"\n","    Fungsi untuk membersihkan dan melakukan preprocessing teks khusus untuk TF-IDF.\n","    - Lowercasing\n","    - Menghapus karakter non-alfanumerik\n","    - Menghapus stopwords\n","    \"\"\"\n","    if not isinstance(text, str):\n","        return \"\"\n","    # Lowercasing\n","    text = text.lower()\n","    # Hapus semua karakter kecuali huruf dan spasi\n","    text = re.sub(r'[^a-z\\s]', ' ', text)\n","    # Tokenisasi dan hapus stopwords\n","    words = text.split()\n","    filtered_words = [word for word in words if word not in stop_words and len(word) > 2] # Hanya ambil kata > 2 huruf\n","    return \" \".join(filtered_words)\n","\n","# Terapkan preprocessing pada kolom 'text_full' dari DataFrame Anda\n","print(\"\\nMelakukan preprocessing teks untuk TF-IDF (stopword removal, dll.)...\")\n","# df_cases sudah dimuat di awal notebook\n","df_cases['text_for_tfidf'] = df_cases['text_full'].apply(preprocess_for_tfidf)\n","print(\"Preprocessing teks selesai.\")\n","print(\"Contoh teks setelah preprocessing:\")\n","print(df_cases[['text_full', 'text_for_tfidf']].head())\n","\n","\n","# --- 2. Membuat dan Menyimpan Matriks TF-IDF & Vectorizer ---\n","\n","# Path untuk menyimpan model TF-IDF\n","TFIDF_VECTORIZER_FILE = os.path.join(PATH_PROCESSED_DATA, \"tfidf_vectorizer.pkl\")\n","TFIDF_MATRIX_FILE = os.path.join(PATH_PROCESSED_DATA, \"tfidf_matrix.npz\")\n","CASE_IDS_TFIDF_FILE = os.path.join(PATH_PROCESSED_DATA, \"case_ids_tfidf.json\")\n","\n","# Inisialisasi TfidfVectorizer\n","# max_df=0.85 -> abaikan kata yang muncul di lebih dari 85% dokumen\n","# min_df=2 -> abaikan kata yang muncul kurang dari 2 kali\n","tfidf_vectorizer = TfidfVectorizer(max_df=0.85, min_df=2, ngram_range=(1, 2))\n","\n","print(\"\\nMembuat matriks TF-IDF untuk case base...\")\n","# Fit dan transform korpus\n","corpus_tfidf = df_cases['text_for_tfidf'].tolist()\n","tfidf_matrix = tfidf_vectorizer.fit_transform(corpus_tfidf)\n","\n","# Simpan urutan case_id yang sesuai dengan matriks\n","case_ids_for_tfidf = df_cases['case_id'].tolist()\n","\n","print(f\"Matriks TF-IDF berhasil dibuat. Shape: {tfidf_matrix.shape}\")\n","\n","# Simpan vectorizer, matriks, dan urutan ID\n","try:\n","    joblib.dump(tfidf_vectorizer, TFIDF_VECTORIZER_FILE)\n","    print(f\"Vectorizer TF-IDF berhasil disimpan ke: {TFIDF_VECTORIZER_FILE}\")\n","\n","    # Untuk menyimpan sparse matrix, gunakan format .npz dari scipy\n","    from scipy.sparse import save_npz\n","    save_npz(TFIDF_MATRIX_FILE, tfidf_matrix)\n","    print(f\"Matriks TF-IDF berhasil disimpan ke: {TFIDF_MATRIX_FILE}\")\n","\n","    with open(CASE_IDS_TFIDF_FILE, 'w') as f:\n","        json.dump(case_ids_for_tfidf, f)\n","    print(f\"Urutan Case IDs untuk TF-IDF disimpan ke: {CASE_IDS_TFIDF_FILE}\")\n","except Exception as e:\n","    print(f\"Error saat menyimpan model TF-IDF: {e}\")\n","\n","\n","# --- 3. Implementasi Fungsi Retrieval TF-IDF ---\n","\n","# Muat kembali model dan matriks yang tersimpan untuk digunakan di dalam fungsi\n","# Ini adalah praktik yang baik, menunjukkan bagaimana sistem akan bekerja setelah training/fitting\n","try:\n","    loaded_tfidf_vectorizer = joblib.load(TFIDF_VECTORIZER_FILE)\n","    from scipy.sparse import load_npz\n","    loaded_tfidf_matrix = load_npz(TFIDF_MATRIX_FILE)\n","    with open(CASE_IDS_TFIDF_FILE, 'r') as f:\n","        loaded_case_ids_for_tfidf = json.load(f)\n","    print(\"\\nModel TF-IDF, matriks, dan ID berhasil dimuat untuk digunakan di fungsi retrieval.\")\n","except Exception as e:\n","    print(f\"Gagal memuat model/matriks TF-IDF yang tersimpan: {e}\")\n","    # Gunakan variabel dari memori sebagai fallback\n","    loaded_tfidf_vectorizer = tfidf_vectorizer\n","    loaded_tfidf_matrix = tfidf_matrix\n","    loaded_case_ids_for_tfidf = case_ids_for_tfidf\n","\n","\n","def retrieve_cases_tfidf(query_text, k=5):\n","    \"\"\"\n","    Menerima query teks, melakukan preprocessing, mengubahnya menjadi vektor TF-IDF,\n","    dan mengembalikan top-k case_id yang paling mirip berdasarkan cosine similarity.\n","    \"\"\"\n","    if not query_text.strip():\n","        print(\"Query teks kosong.\")\n","        return [], []\n","\n","    print(f\"\\nMelakukan retrieval (TF-IDF) untuk query: \\\"{query_text[:100]}...\\\"\")\n","\n","    # 1. Preprocessing query\n","    processed_query = preprocess_for_tfidf(query_text)\n","\n","    # 2. Transform query menjadi vektor TF-IDF menggunakan vectorizer yang sudah di-load\n","    query_vector = loaded_tfidf_vectorizer.transform([processed_query])\n","\n","    # 3. Hitung cosine similarity\n","    similarities = cosine_similarity(query_vector, loaded_tfidf_matrix)[0]\n","\n","    # 4. Dapatkan top-k indices\n","    top_k_indices = np.argsort(similarities)[-k:][::-1]\n","\n","    # 5. Kembalikan case_id dan skor yang sesuai\n","    top_k_case_ids = [loaded_case_ids_for_tfidf[i] for i in top_k_indices]\n","    top_k_scores = [similarities[i] for i in top_k_indices]\n","\n","    print(\"  Top-k hasil retrieval (TF-IDF) (case_id: score):\")\n","    for case_id, score in zip(top_k_case_ids, top_k_scores):\n","        print(f\"    {case_id}: {score:.4f}\")\n","\n","    return top_k_case_ids, top_k_scores\n","\n","\n","# --- 4. Pengujian Awal Fungsi Retrieval TF-IDF ---\n","print(\"\\n--- Pengujian Awal Fungsi Retrieval TF-IDF ---\")\n","\n","# Muat queries.json yang sudah ada\n","try:\n","    with open(QUERIES_JSON_FILE, 'r', encoding='utf-8') as f:\n","        test_queries_for_run = json.load(f)\n","\n","    # Uji dengan query pertama dari file\n","    if test_queries_for_run:\n","        sample_query = test_queries_for_run[0]\n","        print(f\"Menguji dengan Query ID: {sample_query['query_id']}\")\n","        retrieved_ids_tfidf, retrieved_scores_tfidf = retrieve_cases_tfidf(sample_query['query_text'], k=5)\n","        print(f\"  ID Kasus yang berhasil di-retrieve (TF-IDF): {retrieved_ids_tfidf}\")\n","        if sample_query.get('relevant_case_ids'):\n","             print(f\"  Ground truth relevant_case_ids: {sample_query['relevant_case_ids']}\")\n","    else:\n","        print(\"File queries.json kosong atau tidak valid.\")\n","except FileNotFoundError:\n","    print(f\"File {QUERIES_JSON_FILE} tidak ditemukan. Pengujian awal TF-IDF dilewati.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2zarZYkcrOC","executionInfo":{"status":"ok","timestamp":1750945197796,"user_tz":-420,"elapsed":1384,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"outputId":"31ddf929-77ba-46cf-e68c-b1cd0c62c946"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Memulai Pendekatan 1: TF-IDF + Cosine Similarity ---\n","Jumlah stopwords bahasa Indonesia: 758\n","\n","Melakukan preprocessing teks untuk TF-IDF (stopword removal, dll.)...\n","Preprocessing teks selesai.\n","Contoh teks setelah preprocessing:\n","                                           text_full  \\\n","0  Direktori Putusan Mahkamah Agung Republik Indo...   \n","1  Direktori Putusan Mahkamah Agung Republik Indo...   \n","2  Direktori Putusan Mahkamah Agung Republik Indo...   \n","3  Direktori Putusan Mahkamah Agung Republik Indo...   \n","4  Direktori Putusan Mahkamah Agung Republik Indo...   \n","\n","                                      text_for_tfidf  \n","0  direktori putusan mahkamah agung republik indo...  \n","1  direktori putusan mahkamah agung republik indo...  \n","2  direktori putusan mahkamah agung republik indo...  \n","3  direktori putusan mahkamah agung republik indo...  \n","4  direktori putusan mahkamah agung republik indo...  \n","\n","Membuat matriks TF-IDF untuk case base...\n","Matriks TF-IDF berhasil dibuat. Shape: (18, 3376)\n","Vectorizer TF-IDF berhasil disimpan ke: /content/drive/MyDrive/Semester 6/PK/UAS/data/processed/tfidf_vectorizer.pkl\n","Matriks TF-IDF berhasil disimpan ke: /content/drive/MyDrive/Semester 6/PK/UAS/data/processed/tfidf_matrix.npz\n","Urutan Case IDs untuk TF-IDF disimpan ke: /content/drive/MyDrive/Semester 6/PK/UAS/data/processed/case_ids_tfidf.json\n","\n","Model TF-IDF, matriks, dan ID berhasil dimuat untuk digunakan di fungsi retrieval.\n","\n","--- Pengujian Awal Fungsi Retrieval TF-IDF ---\n","Menguji dengan Query ID: Q001\n","\n","Melakukan retrieval (TF-IDF) untuk query: \"Suami meninggalkan istri dan anak lebih dari dua tahun tanpa nafkah dan kabar berita....\"\n","  Top-k hasil retrieval (TF-IDF) (case_id: score):\n","    4: 0.0608\n","    12: 0.0445\n","    23: 0.0344\n","    14: 0.0278\n","    34: 0.0255\n","  ID Kasus yang berhasil di-retrieve (TF-IDF): [4, 12, 23, 14, 34]\n","  Ground truth relevant_case_ids: [2, 4, 10, 12, 14, 22, 23, 29, 34, 35]\n"]}]}]}