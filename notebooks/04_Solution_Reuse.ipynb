{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNMa35Pa7cJxnjfz9HtkaAk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# 04_Solution_Reuse.ipynb\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import os\n","import json\n","import re\n","from collections import Counter\n","import joblib\n","from scipy.sparse import load_npz\n","import nltk\n","nltk.download('stopwords')\n","from google.colab import drive"],"metadata":{"id":"UVdNlLI5iZbG","executionInfo":{"status":"ok","timestamp":1750945736763,"user_tz":-420,"elapsed":92,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"60f7ab80-4d4e-4950-caaa-d85edad376d1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Az-v5MOVibzB","executionInfo":{"status":"ok","timestamp":1750945547002,"user_tz":-420,"elapsed":25199,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"outputId":"52229a8e-440d-4bf0-ba75-d32f168c078d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# --- 1. Konfigurasi Path ---\n","BASE_DRIVE_PATH = \"/content/drive/MyDrive/Semester 6/PK/UAS\" # Sesuaikan jika berbeda\n","PATH_PROCESSED_DATA = os.path.join(BASE_DRIVE_PATH, \"data/processed\")\n","PATH_EVAL_DATA = os.path.join(BASE_DRIVE_PATH, \"data/eval\")\n","PATH_MODELS_CACHE = os.path.join(BASE_DRIVE_PATH, \"models_cache\") # Opsional\n","PATH_RESULTS = os.path.join(BASE_DRIVE_PATH, \"data/results\")\n","os.makedirs(PATH_RESULTS, exist_ok=True)"],"metadata":{"id":"jpNRCbUzidbi","executionInfo":{"status":"ok","timestamp":1750945549041,"user_tz":-420,"elapsed":2042,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# --- 2. Muat Semua Data, Model, dan Embeddings ---\n","print(\"Memuat semua komponen dari tahap sebelumnya...\")\n","\n","# Muat data kasus\n","CASES_REPRESENTED_CSV = os.path.join(PATH_PROCESSED_DATA, \"cases_represented.csv\")\n","df_cases = pd.read_csv(CASES_REPRESENTED_CSV)\n","df_cases.set_index('case_id', inplace=True)\n","\n","# Muat komponen BERT\n","try:\n","    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    MODEL_NAME = 'indobenchmark/indobert-base-p1'\n","    bert_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=PATH_MODELS_CACHE)\n","    bert_model = AutoModel.from_pretrained(MODEL_NAME, cache_dir=PATH_MODELS_CACHE)\n","    bert_model.to(DEVICE)\n","    bert_model.eval()\n","    bert_case_embeddings = np.load(os.path.join(PATH_PROCESSED_DATA, \"case_embeddings_bert.npy\"))\n","    with open(os.path.join(PATH_PROCESSED_DATA, \"case_ids_bert.json\"), 'r') as f:\n","        bert_case_ids = json.load(f)\n","    print(\"Komponen BERT berhasil dimuat.\")\n","except Exception as e:\n","    print(f\"Error saat memuat komponen BERT: {e}. Pastikan Tahap 3 (BERT) sudah dijalankan.\")\n","    exit()\n","\n","# Muat komponen TF-IDF\n","try:\n","    tfidf_vectorizer = joblib.load(os.path.join(PATH_PROCESSED_DATA, \"tfidf_vectorizer.pkl\"))\n","    tfidf_matrix = load_npz(os.path.join(PATH_PROCESSED_DATA, \"tfidf_matrix.npz\"))\n","    with open(os.path.join(PATH_PROCESSED_DATA, \"case_ids_tfidf.json\"), 'r') as f:\n","        tfidf_case_ids = json.load(f)\n","    print(\"Komponen TF-IDF berhasil dimuat.\")\n","except Exception as e:\n","    print(f\"Error saat memuat komponen TF-IDF: {e}. Pastikan Tahap 3 (TF-IDF) sudah dijalankan.\")\n","    exit()\n","\n","# Muat queries\n","QUERIES_JSON_FILE = os.path.join(PATH_EVAL_DATA, \"queries.json\")\n","try:\n","    with open(QUERIES_JSON_FILE, 'r', encoding='utf-8') as f:\n","        test_queries = json.load(f)\n","except FileNotFoundError:\n","    print(f\"Error: File {QUERIES_JSON_FILE} tidak ditemukan. Buat contoh manual.\")\n","    test_queries = [{\"query_id\": \"Q_DEMO_01\", \"query_text\": \"Istri ditinggal suami tanpa nafkah.\"}]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c6_qJ6Veif5K","executionInfo":{"status":"ok","timestamp":1750945592357,"user_tz":-420,"elapsed":43313,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"outputId":"09c98d12-ff55-4017-a0e5-eae044044d50"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Memuat semua komponen dari tahap sebelumnya...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Komponen BERT berhasil dimuat.\n","Komponen TF-IDF berhasil dimuat.\n"]}]},{"cell_type":"code","source":["# --- 3. Definisikan Ulang Fungsi-Fungsi dari Tahap 3 ---\n","# Preprocessing untuk TF-IDF\n","stop_words = list(nltk.corpus.stopwords.words('indonesian'))\n","def preprocess_for_tfidf(text):\n","    if not isinstance(text, str): return \"\"\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', ' ', text)\n","    words = text.split()\n","    return \" \".join([word for word in words if word not in stop_words and len(word) > 2])\n","\n","# Fungsi get_bert_embedding\n","def get_bert_embedding(text, model, tokenizer, device):\n","    encoded_input = tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n","    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n","    with torch.no_grad():\n","        outputs = model(**encoded_input)\n","    attention_mask, last_hidden_states = encoded_input['attention_mask'], outputs.last_hidden_state\n","    mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n","    sum_embeddings = torch.sum(last_hidden_states * mask_expanded, 1)\n","    sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n","    return (sum_embeddings / sum_mask).cpu().numpy().flatten()\n","\n","# Fungsi retrieval TF-IDF\n","def retrieve_cases_tfidf(query_text, k=5):\n","    processed_query = preprocess_for_tfidf(query_text)\n","    query_vector = tfidf_vectorizer.transform([processed_query])\n","    similarities = cosine_similarity(query_vector, tfidf_matrix)[0]\n","    top_k_indices = np.argsort(similarities)[-k:][::-1]\n","    top_k_case_ids = [tfidf_case_ids[i] for i in top_k_indices]\n","    top_k_scores = [similarities[i] for i in top_k_indices]\n","    return top_k_case_ids, top_k_scores\n","\n","# Fungsi retrieval BERT\n","def retrieve_cases_bert(query_text, k=5):\n","    query_embedding = get_bert_embedding(query_text, bert_model, bert_tokenizer, DEVICE).reshape(1, -1)\n","    similarities = cosine_similarity(query_embedding, bert_case_embeddings)[0]\n","    top_k_indices = np.argsort(similarities)[-k:][::-1]\n","    top_k_case_ids = [bert_case_ids[i] for i in top_k_indices]\n","    top_k_scores = [similarities[i] for i in top_k_indices]\n","    return top_k_case_ids, top_k_scores"],"metadata":{"id":"ZOBzyhgViiu5","executionInfo":{"status":"ok","timestamp":1750945742042,"user_tz":-420,"elapsed":48,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# --- 4. Fungsi Prediksi / Solution Reuse (DIMODIFIKASI) ---\n","def classify_amar_outcome(amar_text):\n","    if not isinstance(amar_text, str): return \"TIDAK DIKETAHUI\"\n","    amar_text_lower = amar_text.lower()\n","    if re.search(r\"mengabulkan\\s+gugatan\\s+penggugat\", amar_text_lower): return \"MENGABULKAN GUGATAN\"\n","    if re.search(r\"menolak\\s+gugatan\\s+penggugat\", amar_text_lower): return \"MENOLAK GUGATAN\"\n","    if re.search(r\"menyatakan\\s+gugatan\\s+(?:penggugat|para penggugat)\\s+tidak\\s+dapat\\s+diterima\", amar_text_lower): return \"GUGATAN TIDAK DAPAT DITERIMA (NO)\"\n","    return \"LAIN-LAIN\"\n","\n","def predict_outcome(query_text, k=5, vote_method='weighted_similarity', retrieval_method='bert'):\n","    \"\"\"\n","    Memprediksi 'solusi' untuk query baru berdasarkan kasus serupa.\n","    Dapat menggunakan metode retrieval 'bert' atau 'tfidf'.\n","    \"\"\"\n","    # Langkah 1: Pilih fungsi retrieval yang sesuai\n","    if retrieval_method == 'bert':\n","        top_k_ids, top_k_scores = retrieve_cases_bert(query_text, k)\n","    elif retrieval_method == 'tfidf':\n","        top_k_ids, top_k_scores = retrieve_cases_tfidf(query_text, k)\n","    else:\n","        raise ValueError(\"Metode retrieval tidak valid. Pilih 'bert' atau 'tfidf'.\")\n","\n","    if not top_k_ids:\n","        return \"Tidak ada kasus serupa yang ditemukan\", []\n","\n","    # Langkah 2 & 3: Ekstrak, klasifikasi, dan lakukan voting (logika ini tetap sama)\n","    solutions = [classify_amar_outcome(df_cases.loc[case_id, 'amar_putusan']) for case_id in top_k_ids if case_id in df_cases.index]\n","\n","    predicted_solution = \"Tidak dapat diprediksi\"\n","    if vote_method == 'majority_vote' and solutions:\n","        vote_counts = Counter(solutions)\n","        if vote_counts: predicted_solution = vote_counts.most_common(1)[0][0]\n","\n","    elif vote_method == 'weighted_similarity' and solutions:\n","        weighted_scores = {}\n","        for i, category in enumerate(solutions):\n","            if category not in weighted_scores: weighted_scores[category] = 0\n","            weighted_scores[category] += top_k_scores[i]\n","        if weighted_scores: predicted_solution = max(weighted_scores, key=weighted_scores.get)\n","\n","    return predicted_solution, top_k_ids"],"metadata":{"id":"lHvhDZonipKt","executionInfo":{"status":"ok","timestamp":1750945745830,"user_tz":-420,"elapsed":12,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTW6N71viWXi","executionInfo":{"status":"ok","timestamp":1750945749630,"user_tz":-420,"elapsed":760,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"outputId":"e41143e0-54e6-44cf-a9a9-0cb2730d1ac3"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Demo dan Penyimpanan Hasil Prediksi (Untuk Kedua Model) ---\n","\n","==================== Memproses Query ID: Q001 ====================\n","\n","--- Menggunakan Metode Retrieval: BERT ---\n","-> Top 5 Similar Case IDs: [30, 2, 29, 7, 23]\n","-> Predicted Outcome/Solution: 'LAIN-LAIN'\n","\n","--- Menggunakan Metode Retrieval: TFIDF ---\n","-> Top 5 Similar Case IDs: [4, 12, 23, 14, 34]\n","-> Predicted Outcome/Solution: 'LAIN-LAIN'\n","\n","==================== Memproses Query ID: Q002 ====================\n","\n","--- Menggunakan Metode Retrieval: BERT ---\n","-> Top 5 Similar Case IDs: [2, 30, 22, 23, 12]\n","-> Predicted Outcome/Solution: 'LAIN-LAIN'\n","\n","--- Menggunakan Metode Retrieval: TFIDF ---\n","-> Top 5 Similar Case IDs: [10, 22, 4, 14, 1]\n","-> Predicted Outcome/Solution: 'LAIN-LAIN'\n","\n","==================== Memproses Query ID: Q003 ====================\n","\n","--- Menggunakan Metode Retrieval: BERT ---\n","-> Top 5 Similar Case IDs: [30, 7, 2, 22, 10]\n","-> Predicted Outcome/Solution: 'LAIN-LAIN'\n","\n","--- Menggunakan Metode Retrieval: TFIDF ---\n","-> Top 5 Similar Case IDs: [1, 10, 33, 2, 7]\n","-> Predicted Outcome/Solution: 'LAIN-LAIN'\n","\n","\n","==================================================================\n","Hasil perbandingan prediksi berhasil disimpan ke: /content/drive/MyDrive/Semester 6/PK/UAS/data/results/predictions_comparison.csv\n","Cuplikan hasil perbandingan prediksi:\n","  query_id retrieval_method predicted_solution       top_5_case_ids\n","0     Q001             BERT          LAIN-LAIN   [30, 2, 29, 7, 23]\n","1     Q001            TFIDF          LAIN-LAIN  [4, 12, 23, 14, 34]\n","2     Q002             BERT          LAIN-LAIN  [2, 30, 22, 23, 12]\n","3     Q002            TFIDF          LAIN-LAIN   [10, 22, 4, 14, 1]\n","4     Q003             BERT          LAIN-LAIN   [30, 7, 2, 22, 10]\n","5     Q003            TFIDF          LAIN-LAIN    [1, 10, 33, 2, 7]\n","\n","--- Tahap 4 (Versi Perbandingan) Selesai ---\n"]}],"source":["# --- 5. Demo Manual & Penyimpanan Hasil ---\n","print(\"\\n--- Demo dan Penyimpanan Hasil Prediksi (Untuk Kedua Model) ---\")\n","\n","prediction_results_all = []\n","if test_queries:\n","    for query_data in test_queries:\n","        query_id = query_data['query_id']\n","        query_text = query_data['query_text']\n","        print(f\"\\n==================== Memproses Query ID: {query_id} ====================\")\n","\n","        # Jalankan untuk kedua metode retrieval\n","        for method in ['bert', 'tfidf']:\n","            print(f\"\\n--- Menggunakan Metode Retrieval: {method.upper()} ---\")\n","            predicted_solution, top_5_ids = predict_outcome(query_text, k=5, retrieval_method=method)\n","\n","            print(f\"-> Top 5 Similar Case IDs: {top_5_ids}\")\n","            print(f\"-> Predicted Outcome/Solution: '{predicted_solution}'\")\n","\n","            # Simpan hasil untuk CSV\n","            prediction_results_all.append({\n","                \"query_id\": query_id,\n","                \"retrieval_method\": method.upper(), # Tambahkan kolom ini\n","                \"predicted_solution\": predicted_solution,\n","                \"top_5_case_ids\": json.dumps(top_5_ids)\n","            })\n","else:\n","    print(\"Tidak ada query untuk diuji.\")\n","\n","# Simpan hasil prediksi ke file CSV dengan format baru\n","if prediction_results_all:\n","    df_predictions = pd.DataFrame(prediction_results_all)\n","    PREDICTIONS_CSV_FILE = os.path.join(PATH_RESULTS, \"predictions_comparison.csv\")\n","    df_predictions.to_csv(PREDICTIONS_CSV_FILE, index=False, encoding='utf-8-sig')\n","    print(f\"\\n\\n==================================================================\")\n","    print(f\"Hasil perbandingan prediksi berhasil disimpan ke: {PREDICTIONS_CSV_FILE}\")\n","    print(\"Cuplikan hasil perbandingan prediksi:\")\n","    print(df_predictions)\n","\n","print(\"\\n--- Tahap 4 (Versi Perbandingan) Selesai ---\")"]}]}