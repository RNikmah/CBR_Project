{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOp3Jl/3X/fwa91PFOcgwre"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# 04_Solution_Reuse.ipynb\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.metrics.pairwise import cosine_similarity\n","import os\n","import json\n","import re\n","from collections import Counter\n","from google.colab import drive"],"metadata":{"id":"UVdNlLI5iZbG","executionInfo":{"status":"ok","timestamp":1749369455194,"user_tz":-420,"elapsed":16774,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Az-v5MOVibzB","executionInfo":{"status":"ok","timestamp":1749369478682,"user_tz":-420,"elapsed":23491,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"outputId":"cadd38bc-b244-4d1b-f48a-40e3d8abe305"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# --- Konfigurasi Path (sesuaikan dengan struktur proyek Anda) ---\n","BASE_DRIVE_PATH = \"/content/drive/MyDrive/Semester 6/PK/UAS\" # Sesuaikan jika berbeda\n","PATH_PROCESSED_DATA = os.path.join(BASE_DRIVE_PATH, \"data/processed\")\n","PATH_EVAL_DATA = os.path.join(BASE_DRIVE_PATH, \"data/eval\")\n","PATH_MODELS_CACHE = os.path.join(BASE_DRIVE_PATH, \"models_cache\") # Opsional\n","PATH_RESULTS = os.path.join(BASE_DRIVE_PATH, \"data/results\")\n","\n","# Membuat direktori jika belum ada\n","os.makedirs(PATH_RESULTS, exist_ok=True)\n","\n","# File input dari tahap-tahap sebelumnya\n","CASES_REPRESENTED_CSV = os.path.join(PATH_PROCESSED_DATA, \"cases_represented.csv\")\n","CASE_EMBEDDINGS_FILE = os.path.join(PATH_PROCESSED_DATA, \"case_embeddings_bert.npy\")\n","CASE_IDS_FILE = os.path.join(PATH_PROCESSED_DATA, \"case_ids_bert.json\")\n","QUERIES_JSON_FILE = os.path.join(PATH_EVAL_DATA, \"queries.json\")\n","\n","# File output untuk tahap ini\n","PREDICTIONS_CSV_FILE = os.path.join(PATH_RESULTS, \"predictions.csv\")"],"metadata":{"id":"jpNRCbUzidbi","executionInfo":{"status":"ok","timestamp":1749369481392,"user_tz":-420,"elapsed":2709,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# --- 1. Muat Data, Model, dan Embeddings ---\n","print(\"Memuat data, model, dan embeddings dari tahap sebelumnya...\")\n","\n","# Muat data kasus yang sudah direpresentasikan\n","try:\n","    df_cases = pd.read_csv(CASES_REPRESENTED_CSV)\n","    # Set case_id sebagai index untuk pencarian cepat (lookup)\n","    df_cases.set_index('case_id', inplace=True)\n","except FileNotFoundError:\n","    print(f\"Error: File {CASES_REPRESENTED_CSV} tidak ditemukan. Pastikan Tahap 2 sudah dijalankan.\")\n","    exit()\n","\n","# Muat embeddings dan case_ids\n","try:\n","    case_embeddings = np.load(CASE_EMBEDDINGS_FILE)\n","    with open(CASE_IDS_FILE, 'r') as f:\n","        case_ids_for_embeddings = json.load(f)\n","except FileNotFoundError:\n","    print(f\"Error: File embeddings ({CASE_EMBEDDINGS_FILE}) atau case_ids ({CASE_IDS_FILE}) tidak ditemukan. Pastikan Tahap 3 sudah dijalankan.\")\n","    exit()\n","\n","# Muat Model dan Tokenizer IndoBERT\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","MODEL_NAME = 'indobenchmark/indobert-base-p1'\n","try:\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=PATH_MODELS_CACHE)\n","    model = AutoModel.from_pretrained(MODEL_NAME, cache_dir=PATH_MODELS_CACHE)\n","    model.to(DEVICE)\n","    model.eval()\n","    print(\"Model dan tokenizer berhasil dimuat.\")\n","except Exception as e:\n","    print(f\"Error saat memuat model: {e}\")\n","    exit()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c6_qJ6Veif5K","executionInfo":{"status":"ok","timestamp":1749369516540,"user_tz":-420,"elapsed":35144,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"outputId":"169d3b13-ce32-407f-bcb5-95c65fc336e5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Memuat data, model, dan embeddings dari tahap sebelumnya...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Model dan tokenizer berhasil dimuat.\n"]}]},{"cell_type":"code","source":["# --- 2. Definisikan Ulang Fungsi dari Tahap 3 ---\n","# Kita butuh fungsi get_bert_embedding dan retrieve_cases_bert\n","\n","def get_bert_embedding(text, model, tokenizer, device, max_length=512):\n","    \"\"\"Menghasilkan embedding untuk teks menggunakan model BERT.\"\"\"\n","    if not isinstance(text, str) or text.strip() == \"\":\n","        return torch.zeros(model.config.hidden_size).cpu().numpy()\n","    encoded_input = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_attention_mask=True, return_tensors='pt')\n","    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n","    with torch.no_grad():\n","        outputs = model(**encoded_input)\n","        last_hidden_states = outputs.last_hidden_state\n","    attention_mask = encoded_input['attention_mask']\n","    mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n","    sum_embeddings = torch.sum(last_hidden_states * mask_expanded, 1)\n","    sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n","    mean_pooled = sum_embeddings / sum_mask\n","    return mean_pooled.cpu().numpy().flatten()\n","\n","def retrieve_cases_bert(query_text, k=5):\n","    \"\"\"Fungsi retrieval dari Tahap 3.\"\"\"\n","    if not query_text.strip(): return [], []\n","    query_embedding = get_bert_embedding(query_text, model, tokenizer, DEVICE).reshape(1, -1)\n","    similarities = cosine_similarity(query_embedding, case_embeddings)[0]\n","    top_k_indices = np.argsort(similarities)[-k:][::-1]\n","    top_k_case_ids = [case_ids_for_embeddings[i] for i in top_k_indices]\n","    top_k_scores = [similarities[i] for i in top_k_indices]\n","    return top_k_case_ids, top_k_scores"],"metadata":{"id":"ZOBzyhgViiu5","executionInfo":{"status":"ok","timestamp":1749369516541,"user_tz":-420,"elapsed":6,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# --- 3. Implementasi Algoritma Prediksi / Solution Reuse ---\n","\n","def classify_amar_outcome(amar_text):\n","    \"\"\"\n","    Mengklasifikasikan teks amar putusan ke dalam kategori hasil utama.\n","    Ini adalah pendekatan yang lebih robust daripada membandingkan teks amar secara langsung.\n","    Contoh untuk kasus perceraian. Anda mungkin perlu menyesuaikan pola regex ini.\n","    \"\"\"\n","    if not isinstance(amar_text, str):\n","        return \"TIDAK DIKETAHUI\"\n","\n","    amar_text_lower = amar_text.lower()\n","\n","    # Pola untuk mengabulkan gugatan\n","    if re.search(r\"mengabulkan\\s+gugatan\\s+penggugat\", amar_text_lower):\n","        return \"MENGABULKAN GUGATAN\"\n","\n","    # Pola untuk menolak gugatan\n","    if re.search(r\"menolak\\s+gugatan\\s+penggugat\", amar_text_lower):\n","        return \"MENOLAK GUGATAN\"\n","\n","    # Pola untuk menyatakan gugatan tidak dapat diterima (Niet Ontvankelijke Verklaard / NO)\n","    if re.search(r\"menyatakan\\s+gugatan\\s+(?:penggugat|para penggugat)\\s+tidak\\s+dapat\\s+diterima\", amar_text_lower):\n","        return \"GUGATAN TIDAK DAPAT DITERIMA (NO)\"\n","\n","    # Jika tidak ada yang cocok, kembalikan kategori default\n","    return \"LAIN-LAIN\"\n","\n","\n","def predict_outcome(query_text, k=5, method='majority_vote'):\n","    \"\"\"\n","    Memprediksi 'solusi' untuk query baru berdasarkan kasus-kasus serupa.\n","    Solusi di sini adalah kategori amar putusan.\n","\n","    Args:\n","        query_text (str): Teks kasus baru / query.\n","        k (int): Jumlah kasus serupa yang akan dipertimbangkan.\n","        method (str): 'majority_vote' atau 'weighted_similarity'.\n","\n","    Returns:\n","        tuple: (predicted_solution_category, top_k_case_ids)\n","    \"\"\"\n","    # 1. Temukan top-k kasus serupa\n","    top_k_ids, top_k_scores = retrieve_cases_bert(query_text, k)\n","\n","    if not top_k_ids:\n","        return \"Tidak ada kasus serupa yang ditemukan\", []\n","\n","    # 2. Ekstrak dan klasifikasikan 'solusi' (amar putusan) dari top-k kasus\n","    solutions = []\n","    for case_id in top_k_ids:\n","        try:\n","            amar = df_cases.loc[case_id, 'amar_putusan']\n","            amar_category = classify_amar_outcome(amar)\n","            solutions.append(amar_category)\n","        except KeyError:\n","            print(f\"Peringatan: case_id {case_id} tidak ditemukan di DataFrame. Dilewati.\")\n","            solutions.append(None) # Atau bisa diabaikan\n","\n","    if all(s is None for s in solutions):\n","        return \"Tidak ada solusi yang bisa diekstrak\", top_k_ids\n","\n","    # 3. Terapkan algoritma prediksi\n","    predicted_solution = \"Tidak dapat diprediksi\"\n","    if method == 'majority_vote':\n","        # Hitung frekuensi setiap kategori solusi dan ambil yang paling umum\n","        vote_counts = Counter(s for s in solutions if s is not None)\n","        if vote_counts:\n","            predicted_solution = vote_counts.most_common(1)[0][0]\n","\n","    elif method == 'weighted_similarity':\n","        # Bobot setiap suara dengan skor kemiripannya\n","        weighted_scores = {}\n","        for i, category in enumerate(solutions):\n","            if category is not None:\n","                if category not in weighted_scores:\n","                    weighted_scores[category] = 0\n","                weighted_scores[category] += top_k_scores[i] # Tambahkan bobot skor\n","\n","        if weighted_scores:\n","            # Pilih kategori dengan total skor bobot tertinggi\n","            predicted_solution = max(weighted_scores, key=weighted_scores.get)\n","\n","    else:\n","        raise ValueError(\"Metode tidak valid. Pilih 'majority_vote' atau 'weighted_similarity'.\")\n","\n","    return predicted_solution, top_k_ids"],"metadata":{"id":"lHvhDZonipKt","executionInfo":{"status":"ok","timestamp":1749369516543,"user_tz":-420,"elapsed":6,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTW6N71viWXi","executionInfo":{"status":"ok","timestamp":1749369518084,"user_tz":-420,"elapsed":1545,"user":{"displayName":"Rahmatun Nikmah","userId":"08874952896080255793"}},"outputId":"361b6cfa-e6b8-4443-ba42-116e3395aafd"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Demo dan Penyimpanan Hasil Prediksi ---\n","\n","Memproses Query ID: Q001\n","Query Text: \"Suami meninggalkan istri dan anak lebih dari dua tahun tanpa nafkah dan kabar berita....\"\n","-> Top 5 Similar Case IDs: [30, 2, 29, 7, 23]\n","-> Predicted Outcome/Solution: 'LAIN-LAIN'\n","\n","Memproses Query ID: Q002\n","Query Text: \"Terjadi perselisihan dan pertengkaran terus menerus dalam rumah tangga sehingga tidak ada harapan rukun kembali....\"\n","-> Top 5 Similar Case IDs: [2, 30, 22, 23, 12]\n","-> Predicted Outcome/Solution: 'LAIN-LAIN'\n","\n","Memproses Query ID: Q003\n","Query Text: \"Salah satu pihak melakukan penganiayaan....\"\n","-> Top 5 Similar Case IDs: [30, 7, 2, 22, 10]\n","-> Predicted Outcome/Solution: 'LAIN-LAIN'\n","\n"," Hasil prediksi berhasil disimpan ke: /content/drive/MyDrive/Semester 6/PK/UAS/data/results/predictions.csv\n","Cuplikan hasil prediksi:\n","  query_id predicted_solution       top_5_case_ids\n","0     Q001          LAIN-LAIN   [30, 2, 29, 7, 23]\n","1     Q002          LAIN-LAIN  [2, 30, 22, 23, 12]\n","2     Q003          LAIN-LAIN   [30, 7, 2, 22, 10]\n","\n","--- Tahap 4 Selesai ---\n"]}],"source":["# --- 4. Demo Manual & Penyimpanan Hasil ---\n","print(\"\\n--- Demo dan Penyimpanan Hasil Prediksi ---\")\n","\n","# Muat queries dari Tahap 3\n","try:\n","    with open(QUERIES_JSON_FILE, 'r', encoding='utf-8') as f:\n","        test_queries = json.load(f)\n","except FileNotFoundError:\n","    print(f\"Error: File queries.json tidak ditemukan di {QUERIES_JSON_FILE}.\")\n","    print(\"Membuat contoh query manual untuk demo.\")\n","    test_queries = [{\n","        \"query_id\": \"Q_DEMO_01\",\n","        \"query_text\": \"Seorang istri ditinggal pergi suaminya selama 6 tahun berturut-turut tanpa kabar dan tidak memberikan nafkah lahir batin.\"\n","    }]\n","\n","prediction_results = []\n","if test_queries:\n","    for query_data in test_queries:\n","        query_id = query_data['query_id']\n","        query_text = query_data['query_text']\n","\n","        print(f\"\\nMemproses Query ID: {query_id}\")\n","        print(f\"Query Text: \\\"{query_text[:150]}...\\\"\")\n","\n","        # Lakukan prediksi (kita gunakan weighted_similarity sebagai contoh)\n","        predicted_solution, top_5_ids = predict_outcome(query_text, k=5, method='weighted_similarity')\n","\n","        print(f\"-> Top 5 Similar Case IDs: {top_5_ids}\")\n","        print(f\"-> Predicted Outcome/Solution: '{predicted_solution}'\")\n","\n","        # Simpan hasil untuk CSV\n","        prediction_results.append({\n","            \"query_id\": query_id,\n","            \"predicted_solution\": predicted_solution,\n","            \"top_5_case_ids\": json.dumps(top_5_ids) # Simpan list sebagai string JSON\n","        })\n","else:\n","    print(\"Tidak ada query untuk diuji.\")\n","\n","# Simpan hasil prediksi ke file CSV sesuai instruksi PDF\n","if prediction_results:\n","    df_predictions = pd.DataFrame(prediction_results)\n","    try:\n","        df_predictions.to_csv(PREDICTIONS_CSV_FILE, index=False, encoding='utf-8-sig')\n","        print(f\"\\n Hasil prediksi berhasil disimpan ke: {PREDICTIONS_CSV_FILE}\")\n","        print(\"Cuplikan hasil prediksi:\")\n","        print(df_predictions)\n","    except Exception as e:\n","        print(f\"Error saat menyimpan file prediksi CSV: {e}\")\n","\n","print(\"\\n--- Tahap 4 Selesai ---\")"]}]}